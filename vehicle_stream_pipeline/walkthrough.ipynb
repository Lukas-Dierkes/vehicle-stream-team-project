{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Walkthrough"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from vehicle_stream_pipeline import utils, data_cleaning\n",
    "import pandas as pd\n",
    "import git\n",
    "import os\n",
    "import warnings\n",
    "import time\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "repo = git.Repo(\".\", search_parent_directories=True).git.rev_parse(\n",
    "    \"--show-toplevel\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combing the data\n",
    "First, we combine the monthly excel sheets into mutliple big csv files and store them. We create three seperate files.\n",
    "1. kpi_combined.csv: That is the monthly kpi-stats combined. We rarely use this data \n",
    "\n",
    "2. mtd_combined.csv: That (should) contain all the rides combined for each day of the month according to excel sheet. \n",
    "\n",
    "3. rides_combined: Here we iterated over each day (excel sheet) and collected the data for each day on our own. Suprisingly this is different to the mtd_combined.csv and seems like that this data is more accurate. So we will use this dataframe for further analysis.\n",
    "\n",
    "With this function you can also build a new dataframe when MoD uploaded data for the upcoming months. Just store them in the data/normal_rides folder. \n",
    "\n",
    "(Takes about 20 seconds)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_rides = utils.create_overall_dataframes(f\"{repo}/data/normal_rides\")\n",
    "\n",
    "all_rides[\"df_kpi\"].to_csv(f\"{repo}/data/other/kpi_combined.csv\")\n",
    "all_rides[\"df_mtd\"].to_csv(f\"{repo}/data/other/mtd_combined.csv\")\n",
    "all_rides[\"df_rides\"].to_csv(f\"{repo}/data/other/rides_combined.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "clean id\n",
      "clean distance\n",
      "clean addresses\n",
      "clean free_rides\n",
      "clean created_at\n",
      "clean scheduled_to\n",
      "clean dispatched_at\n",
      "clean vehicle_arrived_at\n",
      "clean arriving_push\n",
      "clean earliest_pickup_expectation\n",
      "clean pickup_at\n",
      "clean pickup_eta\n",
      "clean pickup_first_eta\n",
      "clean dropoff_at\n",
      "clean dropoff_eta\n",
      "clean dropoff_first_eta\n",
      "clean time periods\n",
      "clean rating\n",
      "add shared rides\n",
      "check cleaned data\n"
     ]
    }
   ],
   "source": [
    "# Read in all necessary files for cleaning the data\n",
    "df = pd.read_csv(f\"{repo}/data/rides_combined.csv\", index_col=0)\n",
    "df_stops = pd.read_excel(\n",
    "    f\"{repo}/data/other/MoDstops+Preismodell.xlsx\", sheet_name=\"MoDstops\"\n",
    ")\n",
    "vehicle_usage_df = pd.read_excel(\n",
    "    f\"{repo}/data/vehicle_data/MoD_Vehicle Usage_2021+2022-05-15.xlsx\"\n",
    ")\n",
    "external_df = pd.read_excel(\n",
    "    f\"{repo}/data/vehicle_data/Autofleet_Rides with External ID_2021+2022-05-15.xlsx\"\n",
    ")\n",
    "\n",
    "# Eliminating duplicates\n",
    "df = data_cleaning.clean_duplicates(df)\n",
    "\n",
    "# Clean the data using our cleaning functions\n",
    "df = data_cleaning.data_cleaning(df, df_stops)\n",
    "\n",
    "# Add shared rides to our data pool\n",
    "df = data_cleaning.add_shared_rides(df, vehicle_usage_df, external_df)\n",
    "\n",
    "# Last check if data is correct if some rows are incorrect we store them in a file to analyze them and thus can adapt our cleaning script\n",
    "print(\"check cleaned data\")\n",
    "df, df_incorrect = data_cleaning.data_check(df)\n",
    "if df_incorrect.empty == False:\n",
    "    df_incorrect.to_excel(f\"{repo}/data/cleaning/incorrect{int(time.time())}.xlsx\")\n",
    "\n",
    "# Save our cleaned script. This will be used for the later use cases\n",
    "df.to_csv(f\"{repo}/data/cleaning/data_cleaned.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ride Simulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use case 1: Probablistic graph model - shortest path, drones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pickup_address</th>\n",
       "      <th>dropoff_address</th>\n",
       "      <th>number_of_drives</th>\n",
       "      <th>waiting_time</th>\n",
       "      <th>avg_ride_time</th>\n",
       "      <th>avg_time_to_destination</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1001</td>\n",
       "      <td>1004</td>\n",
       "      <td>4</td>\n",
       "      <td>83.750000</td>\n",
       "      <td>0.001704</td>\n",
       "      <td>83.751704</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1001</td>\n",
       "      <td>1005</td>\n",
       "      <td>19</td>\n",
       "      <td>17.631579</td>\n",
       "      <td>0.001543</td>\n",
       "      <td>17.633122</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1001</td>\n",
       "      <td>1007</td>\n",
       "      <td>2</td>\n",
       "      <td>167.500000</td>\n",
       "      <td>0.002882</td>\n",
       "      <td>167.502882</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1001</td>\n",
       "      <td>1008</td>\n",
       "      <td>3</td>\n",
       "      <td>111.666667</td>\n",
       "      <td>0.003538</td>\n",
       "      <td>111.670204</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1001</td>\n",
       "      <td>1012</td>\n",
       "      <td>2</td>\n",
       "      <td>167.500000</td>\n",
       "      <td>0.005694</td>\n",
       "      <td>167.505694</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   pickup_address  dropoff_address  number_of_drives  waiting_time  \\\n",
       "0            1001             1004                 4     83.750000   \n",
       "1            1001             1005                19     17.631579   \n",
       "2            1001             1007                 2    167.500000   \n",
       "3            1001             1008                 3    111.666667   \n",
       "4            1001             1012                 2    167.500000   \n",
       "\n",
       "   avg_ride_time  avg_time_to_destination  \n",
       "0       0.001704                83.751704  \n",
       "1       0.001543                17.633122  \n",
       "2       0.002882               167.502882  \n",
       "3       0.003538               111.670204  \n",
       "4       0.005694               167.505694  "
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#First we create our aggregated drives so that for every route we calculate the average time to destination\n",
    "cleaned_drives = pd.read_csv(f\"{repo}/data/cleaning/data_cleaned.csv\")\n",
    "cleaned_drives[\"scheduled_to\"] = pd.to_datetime(cleaned_drives[\"scheduled_to\"])\n",
    "start_date = cleaned_drives[\"scheduled_to\"].min()\n",
    "end_date = cleaned_drives[\"scheduled_to\"].max()\n",
    "aggregated_drives = utils.calculate_drives(cleaned_drives, start_date, end_date)\n",
    "aggregated_drives.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<networkx.classes.digraph.DiGraph at 0x11f692c10>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Based on our aggregated drives will build a graph. The weight on an edge between two spots is defined as the avg_time_to_destination\n",
    "graph = utils.calculate_graph(aggregated_drives)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The shortest path uses the following stops [1001, 1005, 4025, 1008, 11017, 1004] and takes 62 days\n"
     ]
    }
   ],
   "source": [
    "path, time_to_destination = utils.get_shortest_ride(1001, 1004, graph)\n",
    "print(f\"The shortest path uses the following stops {path} and takes {int(time_to_destination)} days\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "edges = pd.read_excel(f\"{repo}/data/other/MoDstops+Preismodell.xlsx\", sheet_name= 'Liste 2022')\n",
    "hotspots = utils.get_hotspots(edges, aggregated_drives, n = 10)\n",
    "print(f\"The following spots are the hotspots of our graph: {hotspots}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The following spots are the hotspots of our graph: [1008, 4025, 1005, 1009, 1007, 12007, 7001, 6004, 1010, 11017]'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "drone_spots = [\n",
    "                1008,\n",
    "                4025,\n",
    "                6004,\n",
    "                12007,\n",
    "                11017,\n",
    "                15013,\n",
    "                3021,\n",
    "                8001,\n",
    "                5001,\n",
    "                11003,\n",
    "                4016,\n",
    "            ]\n",
    "utils.add_drone_flights(edges, aggregated_drives, , radius = 500)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
