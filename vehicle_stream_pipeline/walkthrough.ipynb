{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Walkthrough"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from vehicle_stream_pipeline import utils, data_cleaning\n",
    "import pandas as pd\n",
    "import git\n",
    "import os\n",
    "import warnings\n",
    "import time\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "repo = git.Repo(\".\", search_parent_directories=True).git.rev_parse(\n",
    "    \"--show-toplevel\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combing the data\n",
    "First, we combine the monthly excel sheets into mutliple big csv files and store them. We create three seperate files.\n",
    "1. kpi_combined.csv: That is the monthly kpi-stats combined. We rarely use this data \n",
    "\n",
    "2. mtd_combined.csv: That (should) contain all the rides combined for each day of the month according to excel sheet. \n",
    "\n",
    "3. rides_combined: Here we iterated over each day (excel sheet) and collected the data for each day on our own. Suprisingly this is different to the mtd_combined.csv and seems like that this data is more accurate. So we will use this dataframe for further analysis.\n",
    "\n",
    "With this function you can also build a new dataframe when MoD uploaded data for the upcoming months. Just store them in the data/normal_rides folder. \n",
    "\n",
    "(Takes about 20 seconds)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_rides = utils.create_overall_dataframes(f\"{repo}/data/normal_rides\")\n",
    "\n",
    "all_rides[\"df_kpi\"].to_csv(f\"{repo}/data/kpi_combined.csv\")\n",
    "all_rides[\"df_mtd\"].to_csv(f\"{repo}/data/mtd_combined.csv\")\n",
    "all_rides[\"df_rides\"].to_csv(f\"{repo}/data/rides_combined.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "clean id\n",
      "clean distance\n",
      "clean addresses\n",
      "clean free_rides\n",
      "clean created_at\n",
      "clean scheduled_to\n",
      "clean dispatched_at\n",
      "clean vehicle_arrived_at\n",
      "clean arriving_push\n",
      "clean earliest_pickup_expectation\n",
      "clean pickup_at\n",
      "clean pickup_eta\n",
      "clean pickup_first_eta\n",
      "clean dropoff_at\n",
      "clean dropoff_eta\n",
      "clean dropoff_first_eta\n",
      "clean time periods\n",
      "clean rating\n",
      "add shared rides\n",
      "check cleaned data\n"
     ]
    }
   ],
   "source": [
    "# Read in all necessary files for cleaning the data\n",
    "df = pd.read_csv(f\"{repo}/data/rides_combined.csv\", index_col=0)\n",
    "df_stops = pd.read_excel(\n",
    "    f\"{repo}/data/other/MoDstops+Preismodell.xlsx\", sheet_name=\"MoDstops\"\n",
    ")\n",
    "vehicle_usage_df = pd.read_excel(\n",
    "    f\"{repo}/data/vehicle_data/MoD_Vehicle Usage_2021+2022-05-15.xlsx\"\n",
    ")\n",
    "external_df = pd.read_excel(\n",
    "    f\"{repo}/data/vehicle_data/Autofleet_Rides with External ID_2021+2022-05-15.xlsx\"\n",
    ")\n",
    "\n",
    "# Eliminating duplicates\n",
    "df = data_cleaning.clean_duplicates(df)\n",
    "\n",
    "# Clean the data using our cleaning functions\n",
    "df = data_cleaning.data_cleaning(df, df_stops)\n",
    "\n",
    "# Add shared rides to our data pool\n",
    "df = data_cleaning.add_shared_rides(df, vehicle_usage_df, external_df)\n",
    "\n",
    "# Last check if data is correct if some rows are incorrect we store them in a file to analyze them and thus can adapt our cleaning script\n",
    "print(\"check cleaned data\")\n",
    "df, df_incorrect = data_cleaning.data_check(df)\n",
    "if df_incorrect.empty == False:\n",
    "    df_incorrect.to_excel(f\"{repo}/data/cleaning/incorrect{int(time.time())}.xlsx\")\n",
    "\n",
    "# Save our cleaned script. This will be used for the later use cases\n",
    "df.to_csv(f\"{repo}/data/cleaning/data_cleaned.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ride Simulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use case 1: Probablistic graph model - shortest path, drones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
